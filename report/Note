Note1: 
Several model architectures are explicitly designed to trade off accuracy for improved performance or reduced energy consumption. Examples include:

Dynamic Neural Networks, Anytime prediction models, Models with early-exit mechanisms (e.g., BranchyNet, Shallow-Deep Networks), Quantized or pruned variants of a base model.

In such models, reducing the computational load—typically measured in floating-point operations (FLOPs)—can involve techniques such as skipping certain layers, operating at lower numerical precision, or terminating inference early. These adjustments often lead to reduced accuracy.

However, in this work, we consider a different setting. We assume that the model is fixed and already trained, and its inference accuracy is constant (ResNet, MobileNet, YOLO). The number of FLOPs required for a single inference is thus also fixed. Under this assumption, increasing the FLOPs-per-second allocated to the model (e.g., through more powerful hardware or parallelization) reduces inference time, but does not affect accuracy.



Note2:
This study addresses the Quality of Service (QoS) requirements of IoT applications in an edge computing system comprising multiple edge servers. It simultaneously considers both the inference time and the expected average accuracy of inference requests, while also striving to preserve energy efficiency.

To estimate inference time, we use FLOPs (the number of floating-point operations) associated with deep neural networks. Additionally, we consider FLOPs-per-second as a measure of hardware performance and FLOPs-per-watt to account for energy efficiency.

Our initial model selection and resource provisioning strategy follows a greedy-based approach. It aims to deploy as many replicas as possible with minimal energy consumption, while adhering to the resource limitations of edge servers and satisfying the QoS constraints of incoming requests.

In the current experiments, we use the FLOPs values for YOLOv5 deep neural network models as reported by Ultralytics YOLOv5 documentation. The FLOPs-per-second and FLOPs-per-watt values are extracted from the following study:

Desislavov, R., Martínez-Plumed, F., & Hernández-Orallo, J. (2023). Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning. Sustainable Computing: Informatics and Systems, 38, 100857.

The most important metrics as outs are :
- Number of completed (and rejected) requests
- Energy consumption of the edge servers (in total) (Watt)

Also, we can have the following metrics:
- Response time of completed requests (individual requests)
- difference between response time and QoS-deadline of request sets
- difference between estimated average accuracy and expected accuracy of request sets


